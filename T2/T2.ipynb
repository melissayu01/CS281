{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "seaborn.set_context(\"talk\")\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.\n",
    "\"\"\"\n",
    "def p12():\n",
    "    x = np.linspace(0, 15, 1000)\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "    deg_of_freedom = [1, 2, 5, 10, 25, 50, 100]\n",
    "    for df in deg_of_freedom:\n",
    "      ax.plot(x, stats.chi.pdf(x, df), label=r'$df=%i$' % df)\n",
    "\n",
    "    plt.xlim(0, 15)\n",
    "    plt.ylim(0, 1)\n",
    "\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel(r'$f(x)$')\n",
    "    plt.title(r'$\\chi\\ \\mathrm{Distribution}$')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def p13(df):\n",
    "    x = np.linspace(stats.chi.ppf(0.001, df), stats.chi.ppf(0.999, df), 100)\n",
    "    rv = stats.chi(df)\n",
    "        \n",
    "    plt.figure()\n",
    "    plt.plot(x, rv.cdf(x))\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel(r'$F(x)$')\n",
    "    plt.title(r'$\\chi\\ \\mathrm{Distribution}$')\n",
    "    plt.show()\n",
    "\n",
    "p12()\n",
    "p13(df=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4.\n",
    "\"\"\" \n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, alpha, beta, n_features):\n",
    "        # 1 x C vector; dirichlet prior for class distr.\n",
    "        self.alpha = alpha \n",
    "        self.alpha0 = sum(alpha)\n",
    "        \n",
    "        # 1 x K vector; dirichlet prior for class conditional distr.\n",
    "        self.beta = beta   \n",
    "        self.beta0 = sum(beta)\n",
    "        \n",
    "        # dimensions of data\n",
    "        self.C = len(self.alpha)\n",
    "        self.K = len(self.beta)\n",
    "        self.D = n_features\n",
    "        \n",
    "        # counts\n",
    "        self.N = 0\n",
    "        self.N_c = np.zeros(self.C, dtype=int)\n",
    "        self.N_cj = np.zeros((self.C, self.D), dtype=int)\n",
    "        self.N_ckj = np.zeros((self.C, self.K, self.D), dtype=int)\n",
    "        \n",
    "        self.flushed = False\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X = X.astype(int)\n",
    "        N, _D = X.shape\n",
    "        self.N += N\n",
    "        \n",
    "        print(\"Fitting model\")\n",
    "        for c in range(self.C):\n",
    "            msk = y == c\n",
    "            self.N_c[c] += np.sum(msk)\n",
    "            self.N_cj[c] += np.sum(X[msk], dtype=int, axis=0)\n",
    "            self.N_ckj[c] += np.apply_along_axis(np.bincount, 0, X[msk], minlength=self.K)\n",
    "\n",
    "        self.flushed = False\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X = X.astype(int)\n",
    "        \n",
    "        if not self.flushed:\n",
    "            print(\"Flushing\")\n",
    "            self.pi = np.array([\n",
    "                np.log(self.N_c[c] + self.alpha[c]) - np.log(self.N + self.alpha0)\n",
    "                for c in range(self.C)])\n",
    "            self.mu = np.fromfunction(\n",
    "                lambda c, j, k: np.log(self.N_ckj[c, k, j] + self.beta[c]) - np.log(self.N_c[c] + self.beta0),\n",
    "                (self.C, self.D, self.K), dtype=int\n",
    "            )\n",
    "            self.flushed = True\n",
    "        \n",
    "        print(\"Predicting labels\")\n",
    "        p_for_x = lambda x: [self.pi[c] + np.sum([self.mu[c, j, x[j]] for j in range(len(x))]) for c in range(self.C)]\n",
    "        ps = np.apply_along_axis(p_for_x, 1, X)\n",
    "        return np.apply_along_axis(np.argmax, 1, ps)\n",
    "        \n",
    "import utils\n",
    "    \n",
    "def p4():\n",
    "    # load data\n",
    "    train_iter, val_iter, test_iter, text_field = utils.load_imdb(batch_size=1000)\n",
    "    \n",
    "    # initialize classifier\n",
    "    alpha = np.ones(2)\n",
    "    beta = np.ones(281 + 1)\n",
    "    n_features = 245703\n",
    "    nb = NaiveBayesClassifier(alpha, beta, n_features)\n",
    "    \n",
    "    # train\n",
    "    i = 0\n",
    "    for batch in train_iter:\n",
    "        if i > 1: break\n",
    "        print(i)\n",
    "        X = utils.bag_of_words(batch, text_field).data.numpy()\n",
    "        y = batch.label.data.numpy() - 1\n",
    "        nb.fit(X, y)\n",
    "        i += 1\n",
    "\n",
    "    # test\n",
    "    n, n_corr = 0, 0\n",
    "    i = 0\n",
    "    for batch in test_iter:\n",
    "        print(i)\n",
    "        \n",
    "        X = utils.bag_of_words(batch, text_field).data.numpy()\n",
    "        y_pred = nb.predict(X)\n",
    "        y = batch.label.data.numpy() - 1     \n",
    "        \n",
    "        n += len(y)\n",
    "        n_err += sum(abs(y_pred - y))       \n",
    "        i += 1\n",
    "        print(1 - n_err / n)\n",
    "    \n",
    "    return nb, 1 - n_err / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5. & 6.\n",
    "\"\"\"\n",
    "import utils\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "from collections import OrderedDict\n",
    "\n",
    "def build_model(input_dim, output_dim):\n",
    "    model = torch.nn.Sequential(OrderedDict([\n",
    "                (\"linear\", torch.nn.Linear(input_dim, output_dim)), # computes w_c^T x + b_c \n",
    "#                 ('tanh', torch.nn.Tanh()), # tanh\n",
    "                ('relu', torch.nn.ReLU()), # relu\n",
    "                (\"softmax\", torch.nn.LogSoftmax()) # log softmax term\n",
    "            ]))\n",
    "    return model\n",
    "\n",
    "def train(model, loss, reg_weight, optimizer, x_val, y_val):\n",
    "    # Take in x and y and make variable.\n",
    "    x = Variable(x_val)\n",
    "    y = Variable(y_val)\n",
    "\n",
    "    # Resets the gradients to 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Computes the function above. (log softmax w_c^T x + b_c)\n",
    "    fx = model.forward(x)\n",
    "\n",
    "    # Computes loss. Gives a scalar. \n",
    "    output = loss.forward(fx, y)\n",
    "    l1_crit = torch.nn.L1Loss(size_average=False)\n",
    "    target = Variable(torch.zeros(2,245703), requires_grad=False)\n",
    "    param = next(model.parameters())\n",
    "    reg_loss = l1_crit(param, target)\n",
    "    output += reg_weight * reg_loss\n",
    "\n",
    "    # Magically computes the gradients. \n",
    "    output.backward()\n",
    "\n",
    "    # updates the weights\n",
    "    optimizer.step()\n",
    "    return output.data[0]\n",
    "\n",
    "def predict(model, x_val):\n",
    "    x = Variable(x_val, requires_grad=False)\n",
    "    output = model.forward(x)\n",
    "    return output.data.numpy().argmax(axis=1)\n",
    "\n",
    "def main():\n",
    "    torch.manual_seed(42)\n",
    "    n_features = 245703\n",
    "    n_classes = 2\n",
    "    reg_weights = [0,0.001,0.01,0.1,1]\n",
    "    \n",
    "    for reg_weight in reg_weights:\n",
    "        train_iter, val_iter, test_iter, text_field = utils.load_imdb(batch_size=1000)\n",
    "\n",
    "        # build model\n",
    "        model = build_model(n_features, n_classes)\n",
    "\n",
    "        # Loss here is negative log-likelihood \n",
    "        loss = torch.nn.NLLLoss(size_average=True)\n",
    "\n",
    "        # Optimizer. SGD stochastic gradient. \n",
    "        optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "        cost = 0.\n",
    "        num_batches = 0\n",
    "        for batch in train_iter:\n",
    "            X = utils.bag_of_words(batch, text_field).data\n",
    "            y = batch.label.data - 1\n",
    "            cost += train(model, loss, reg_weight, optimizer, X, y)\n",
    "            num_batches += 1\n",
    "            \n",
    "        n, n_corr = 0, 0\n",
    "        for batch in test_iter:\n",
    "            X = utils.bag_of_words(batch, text_field).data\n",
    "            y = batch.label.data - 1\n",
    "            y_pred = predict(model, X)\n",
    "            \n",
    "            n += len(y.numpy())\n",
    "            n_corr += sum(y_pred == y.numpy())\n",
    "\n",
    "        print(\"Lambda %f, cost = %f, acc = %.2f%%\"\n",
    "              % (reg_weight, cost / num_batches, 100. * n_corr / n))\n",
    "        weights = next(model.parameters()).data.numpy()\n",
    "        heaviest = [[text_field.vocab.itos[word_id] for word_id in np.argsort(w)[-5:][::-1]] for w in weights]\n",
    "        lightest = [[text_field.vocab.itos[word_id] for word_id in np.argsort(w)[:5][::-1]] for w in weights]\n",
    "        sparsity = np.sum(np.abs(weights) < 1e-4) / (weights.shape[0] * weights.shape[1])\n",
    "        print(\">>> Heaviest Words\\nClass 0: {}\\nClass 1: {}\".format(heaviest[0], heaviest[1]))\n",
    "        print(\">>> Lightest Words\\nClass 0: {}\\nClass 1: {}\".format(lightest[0], lightest[1]))\n",
    "        print(\">>> Sparsity: {}\".format(sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data\n",
      "2\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/myu/anaconda/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:71: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "/Users/myu/anaconda/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:78: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda 0.000000, cost = 0.591833, acc = 85.40%\n",
      ">>> Heaviest Words\n",
      "Class 0: ['boring', 'wasted', 'waste', 'guess', 'horrible']\n",
      "Class 1: ['excellent', 'entertaining', 'favorite', 'loved', 'shows']\n",
      ">>> Lightest Words\n",
      "Class 0: ['great.', 'excellent', 'wonderful.', 'favorite', 'loved']\n",
      "Class 1: ['poor', 'save', 'boring', 'horrible', 'waste']\n",
      ">>> Sparsity: 0.011467096453848752\n",
      "Loading Data\n",
      "2\n",
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-58ca95c5b364>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-f070651842eb>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbag_of_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_field\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mcost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mnum_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-f070651842eb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loss, reg_weight, optimizer, x_val, y_val)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Computes the function above. (log softmax w_c^T x + b_c)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mfx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Computes loss. Gives a scalar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/myu/anaconda/envs/python3/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/myu/anaconda/envs/python3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/myu/anaconda/envs/python3/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/myu/anaconda/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/myu/anaconda/envs/python3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36maddmm\u001b[0;34m(cls, *args)\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAddmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/myu/anaconda/envs/python3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36m_blas\u001b[0;34m(cls, args, inplace)\u001b[0m\n\u001b[1;32m    918\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m                 \u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/myu/anaconda/envs/python3/lib/python3.6/site-packages/torch/autograd/_functions/blas.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, add_matrix, matrix1, matrix2, alpha, beta, inplace)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         return torch.addmm(alpha, add_matrix, beta,\n\u001b[0;32m---> 26\u001b[0;31m                            matrix1, matrix2, out=output)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [python3]",
   "language": "python",
   "name": "Python [python3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
