{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from scipy.special import logsumexp\n",
    "import numpy as np\n",
    "import itertools\n",
    "import torch\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "seaborn.set_context(\"talk\")\n",
    "torch.set_printoptions(precision=4)\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class P1:\n",
    "    def __init__(self, fname):\n",
    "        self.x = np.load(fname)\n",
    "        self.N, self.M, self.K = self.x.shape\n",
    "                        \n",
    "        # log potentials\n",
    "        def edge_func(i, j):\n",
    "            if i == j: \n",
    "                return 10\n",
    "            elif abs(i - j) == 1: \n",
    "                return 2\n",
    "            else: \n",
    "                return 0\n",
    "        self.edge_theta = np.fromfunction(np.vectorize(edge_func), (self.K, self.K))\n",
    "        self.node_theta = 10 * self.x\n",
    "        \n",
    "        # MF\n",
    "        mu = np.zeros(self.x.shape) # mu[i, j, k] = q_ij(y_ij = k)\n",
    "        \n",
    "        # LBP\n",
    "        msgs = None\n",
    "        prev_msgs = None\n",
    "        bels = None\n",
    "    \n",
    "    ### helper functions\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        # normalizes vector x\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "\n",
    "    def neighbors(self, i, j):\n",
    "        out = []\n",
    "        for (di, dj) in [(0, -1), (-1, 0), (0, 1), (1, 0)]:\n",
    "            i_, j_ = i + di, j + dj\n",
    "            if 0 <= i_ < self.N and 0 <= j_ < self.M and (i_, j_) != (i, j):\n",
    "                out.append((i_, j_))\n",
    "        return out\n",
    "\n",
    "    def nodes(self): \n",
    "        return itertools.product(range(self.N), range(self.M))\n",
    "    \n",
    "    def dir_edges(self):\n",
    "        for t in self.nodes():\n",
    "            for n in self.neighbors(*t):\n",
    "                yield (t, n)\n",
    "\n",
    "    def from_one_hot(self, one_hot_vec):\n",
    "        return np.where(one_hot_vec)[0][0]\n",
    "                \n",
    "    ### inference\n",
    "    \n",
    "    def brute_force(self):\n",
    "        def unnormed_log_joint(y):\n",
    "            p = sum([self.node_theta[t].dot(y[t]) for t in self.nodes()])\n",
    "            p += 0.5 * sum(( \n",
    "                self.edge_theta[self.from_one_hot(y[n]), self.from_one_hot(y[t])]\n",
    "                for (n, t) in self.dir_edges() \n",
    "            ))\n",
    "            return p\n",
    "        \n",
    "        def all_ys(key=None):\n",
    "            tuple_to_mat = lambda t: np.asarray(t).reshape((self.N, self.M))\n",
    "            to_one_hot = lambda a: (a[...,None] == np.arange(self.K)).astype(float)\n",
    "            arrays = map(tuple_to_mat, list(itertools.product(range(self.K), repeat=self.N*self.M)))\n",
    "            sorted_arrays = sorted(map(to_one_hot, arrays), key=key)\n",
    "            return itertools.groupby(sorted_arrays, key=key)\n",
    "        \n",
    "        marginals = np.zeros(self.x.shape)\n",
    "        for t in self.nodes():\n",
    "            ps = []\n",
    "            for k, group in all_ys( key=lambda img: self.from_one_hot(img[t]) ):\n",
    "                p = logsumexp([ unnormed_log_joint(y) for y in group ])\n",
    "                ps.append(p)\n",
    "            marginals[t] = self.softmax(ps) # np.exp(ps - logsumexp(ps))\n",
    "        return marginals\n",
    "    \n",
    "    def mean_field(self, epochs=30):\n",
    "        history = [self.x]\n",
    "        prev_mu = self.softmax(np.zeros(self.x.shape)) \n",
    "        for epoch in range(epochs):\n",
    "            mu = np.zeros(self.x.shape)\n",
    "            for (i, j) in self.nodes():\n",
    "                ngh = sum(( prev_mu[t] for t in self.neighbors(i, j) ))\n",
    "                mu[i, j] = self.softmax(self.node_theta[i, j] + self.edge_theta.dot(ngh))\n",
    "            prev_mu = mu\n",
    "            history.append(prev_mu)\n",
    "        return history\n",
    "    \n",
    "    def loopy_bp(self, epochs=30):\n",
    "        history = [self.x]\n",
    "        prev_msgs = {edge: np.zeros(self.K) for edge in self.dir_edges()}\n",
    "        bels = np.ones(self.x.shape)\n",
    "        for epoch in range(epochs):\n",
    "            msgs = {edge: np.zeros(self.K) for edge in self.dir_edges()}\n",
    "            for ((i, j), t) in self.dir_edges():\n",
    "                for k in range(self.K):\n",
    "                    ngh = lambda l: [\n",
    "                        prev_msgs[(u, (i, j))][l] \n",
    "                        for u in self.neighbors(i, j) if u != t \n",
    "                    ]\n",
    "                    values = [\n",
    "                        self.node_theta[i, j, l] + self.edge_theta[k, l] + sum(ngh(l))\n",
    "                        for l in range(self.K)\n",
    "                    ]\n",
    "                    msgs[((i, j), t)][k] = logsumexp(values)\n",
    "            for s in self.nodes():\n",
    "                ngh = (msgs[(t, s)] for t in self.neighbors(*s))\n",
    "                bels[s] = self.softmax(self.node_theta[s] + sum(ngh))\n",
    "            prev_msgs = msgs\n",
    "            history.append(bels)\n",
    "        return history\n",
    "\n",
    "print('small')\n",
    "p1 = P1('small.npy')\n",
    "marginals = p1.brute_force()\n",
    "print(marginals)\n",
    "plt.figure()\n",
    "plt.imshow(marginals)\n",
    "\n",
    "files = ('small', ) # 'flag', 'bullseye', 'spiral'\n",
    "for fname in files:\n",
    "    print(fname)\n",
    "    p1 = P1(fname + '.npy')\n",
    "    for f in (p1.mean_field, p1.loopy_bp):\n",
    "        history = f()\n",
    "        for img in (history[-1], ):\n",
    "            print(img)\n",
    "            plt.figure()\n",
    "            plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2.\n",
    "\"\"\"\n",
    "import utils\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "torch.set_printoptions(precision=4)\n",
    "\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LatentLinearModel(torch.nn.Module):\n",
    "    def __init__(self, K, N, J):\n",
    "        super(LatentLinearModel, self).__init__()\n",
    "        self.K = K\n",
    "        self.N = N\n",
    "        self.J = J\n",
    "\n",
    "        self.mu_U = torch.nn.Embedding(N, K)\n",
    "        self.logvar_U = torch.nn.Embedding(N, K)\n",
    "        self.logvar_U.weight.data.fill_(-10)\n",
    "\n",
    "        self.mu_V = torch.nn.Embedding(J, K)\n",
    "        self.logvar_V = torch.nn.Embedding(J, K)\n",
    "        self.logvar_V.weight.data.fill_(-10)\n",
    "\n",
    "    def forward(self, users, jokes):\n",
    "        batch_size = users.size()[0]\n",
    "        z_U = Variable(torch.from_numpy(np.random.normal(0, 1, (batch_size, self.K))).float(), requires_grad=False)\n",
    "        z_V = Variable(torch.from_numpy(np.random.normal(0, 1, (batch_size, self.K))).float(), requires_grad=False)\n",
    "\n",
    "        u = ( z_U.mul(self.logvar_U(users).exp().sqrt()).add(self.mu_U(users)) ).unsqueeze(1)\n",
    "        v = ( z_V.mul(self.logvar_V(jokes).exp().sqrt()).add(self.mu_V(jokes)) ).unsqueeze(2)\n",
    "        r = torch.bmm(u, v).squeeze()\n",
    "        return r\n",
    "\n",
    "def vi_loss(model, users, jokes):\n",
    "    batch_size = users.size()[0]\n",
    "    var_for_prior = Variable(\n",
    "        torch.from_numpy(5 * np.ones((batch_size, model.K))).float(),\n",
    "        requires_grad=False\n",
    "    )\n",
    "    loss = (0.5 * (var_for_prior.sqrt() - model.logvar_U(users))\n",
    "        + (model.logvar_U(users).exp() + model.mu_U(users) ** 2) / 10).sum()\n",
    "    loss += (0.5 * (var_for_prior.sqrt() - model.logvar_V(jokes))\n",
    "        + (model.logvar_V(jokes).exp() + model.mu_V(jokes) ** 2) / 10).sum()\n",
    "\n",
    "    return loss\n",
    "\n",
    "Ks = range(10, 11)\n",
    "\n",
    "for K in Ks:\n",
    "    train_logprobs = []\n",
    "    train_lower_bound = []\n",
    "    test_logprobs = []\n",
    "    test_loss = []\n",
    "\n",
    "    # Load data iterators\n",
    "    train_iter, val_iter, test_iter = utils.load_jester(\n",
    "        batch_size=100, subsample_rate=0.1, load_text=False)\n",
    "\n",
    "    # Construct our model by instantiating the class defined above\n",
    "    N, J = 70000, 150\n",
    "    model = LatentLinearModel(K, N, J)\n",
    "\n",
    "    # Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "    # in the SGD constructor will contain the learnable parameters of the two\n",
    "    # nn.Linear modules which are members of the model.\n",
    "    criterion = torch.nn.MSELoss(size_average=False)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.04)\n",
    "\n",
    "    # Optimize model parameters\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        lower_bound = 0\n",
    "        logprob_for_epoch = 0\n",
    "        n_in_epoch = 0\n",
    "        U_sample = model.mu_U\n",
    "        V_sample = model.mu_V\n",
    "\n",
    "        train_iter.init_epoch()\n",
    "        for batch in train_iter:\n",
    "            # 0-index data\n",
    "            ratings = (batch.ratings-1).float()\n",
    "            users = batch.users-1\n",
    "            jokes = batch.jokes-1\n",
    "\n",
    "            n_in_epoch += ratings.size()[0]\n",
    "\n",
    "            # Forward pass: Compute predicted y by passing x to the model\n",
    "            r_pred = model(users, jokes)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = 0.5 * criterion(r_pred, ratings)\n",
    "            loss += vi_loss(model, users, jokes) # add entropy and prior terms\n",
    "            lower_bound -= loss.data[0]\n",
    "\n",
    "            # Update log-likelihood for epoch\n",
    "            _r_pred = torch.bmm((U_sample(users)).unsqueeze(1), (V_sample(jokes)).unsqueeze(2)).squeeze()\n",
    "            _logprob = - 0.5 * criterion(_r_pred, ratings) - ratings.size()[0] * np.log(1/(2 * math.pi))\n",
    "            logprob_for_epoch += _logprob.data[0]\n",
    "\n",
    "            # Zero gradients, perform a backward pass, and update the weights\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        lower_bound /= n_in_epoch\n",
    "        logprob_for_epoch /= n_in_epoch\n",
    "        train_logprobs.append(logprob_for_epoch)\n",
    "        train_lower_bound.append(lower_bound)\n",
    "\n",
    "        # Predict on test set\n",
    "        test_logprob_for_epoch = 0\n",
    "        n = 0\n",
    "        for batch in test_iter:\n",
    "            # 0-index data\n",
    "            ratings = (batch.ratings-1).float()\n",
    "            users = batch.users-1\n",
    "            jokes = batch.jokes-1\n",
    "\n",
    "            n += ratings.size()[0]\n",
    "\n",
    "            # Forward pass: Compute predicted y by passing x to the model\n",
    "            r_pred = model(users, jokes)\n",
    "\n",
    "            # Update log-likelihood for epoch\n",
    "            _r_pred = torch.bmm((U_sample(users)).unsqueeze(1), (V_sample(jokes)).unsqueeze(2)).squeeze()\n",
    "            _logprob = - 0.5 * criterion(_r_pred, ratings) - ratings.size()[0] * np.log(1/(2 * math.pi))\n",
    "            test_logprob_for_epoch += _logprob.data[0]\n",
    "\n",
    "        test_logprob_for_epoch /= n\n",
    "        test_logprobs.append(test_logprob_for_epoch)\n",
    "\n",
    "        # Print summary for epoch\n",
    "        print(epoch, logprob_for_epoch, lower_bound)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(num_epochs), train_logprobs, c='r')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel('Training Log-likelihood')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(num_epochs), test_logprobs, c='b')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel('Testing Log-likelihood')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(num_epochs), train_lower_bound, c='g')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel('Training Lower Bound')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [python3]",
   "language": "python",
   "name": "Python [python3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
